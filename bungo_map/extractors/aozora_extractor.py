#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é’ç©ºæ–‡åº«ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå™¨
å®Ÿéš›ã®é’ç©ºæ–‡åº«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã—ã€åœ°åæŠ½å‡ºå¯èƒ½ãªå½¢ã«æ­£è¦åŒ–
"""

import requests
import re
import time
import zipfile
import io
import os
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, quote
from bs4 import BeautifulSoup
import json


class AozoraExtractor:
    """é’ç©ºæ–‡åº«ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå™¨"""
    
    def __init__(self, cache_dir: str = "data/aozora_cache"):
        self.base_url = "https://www.aozora.gr.jp"
        self.api_url = "https://pubserver1.herokuapp.com/api/v0.1/books"
        self.cache_dir = cache_dir
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'BungoMapBot/2.0 (Educational Research Purpose)'
        })
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(cache_dir, exist_ok=True)
        
        # APIã®åˆ©ç”¨å¯èƒ½æ€§ç¢ºèª
        self.api_available = self._check_api_availability()
        
    def _check_api_availability(self) -> bool:
        """é’ç©ºæ–‡åº«APIã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            response = self.session.get(f"{self.api_url}?limit=1", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def search_aozora_work(self, work_title: str, author_name: str) -> Optional[str]:
        """é’ç©ºæ–‡åº«ã§ä½œå“ã®URLã‚’æ¤œç´¢"""
        print(f"ğŸ” é’ç©ºæ–‡åº«æ¤œç´¢: {author_name} - {work_title}")
        
        # APIãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ä½¿ç”¨
        if self.api_available:
            return self._search_via_api(work_title, author_name)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥æ¤œç´¢
        return self._search_via_direct(work_title, author_name)
    
    def _search_via_api(self, work_title: str, author_name: str) -> Optional[str]:
        """APIçµŒç”±ã§ä½œå“æ¤œç´¢"""
        try:
            # ä½œè€…åã§æ¤œç´¢
            params = {'author': author_name, 'limit': 20}
            response = self.session.get(self.api_url, params=params, timeout=10)
            response.raise_for_status()
            
            books = response.json()
            
            # ä½œå“åã§ãƒãƒƒãƒãƒ³ã‚°
            for book in books:
                book_title = book.get('title', '')
                if work_title in book_title or book_title in work_title:
                    text_url = book.get('text_url')
                    if text_url:
                        print(f"âœ… APIæ¤œç´¢æˆåŠŸ: {book_title}")
                        return text_url
            
            print(f"âŒ APIæ¤œç´¢ã§è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {work_title}")
            return None
            
        except Exception as e:
            print(f"âš ï¸ APIæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _search_via_direct(self, work_title: str, author_name: str) -> Optional[str]:
        """ç›´æ¥æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        # ç°¡æ˜“çš„ãªä½œå“URLãƒ‘ã‚¿ãƒ¼ãƒ³
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯é’ç©ºæ–‡åº«ã®ã‚µã‚¤ãƒˆæ§‹é€ ã«åŸºã¥ãæ¤œç´¢ãŒå¿…è¦
        
        # ä¸€èˆ¬çš„ãªé’ç©ºæ–‡åº«ã®ãƒ†ã‚­ã‚¹ãƒˆURLå½¢å¼ã‚’è©¦è¡Œ
        possible_urls = [
            f"https://www.aozora.gr.jp/cards/{author_name}/{work_title}.txt",
            f"https://raw.githubusercontent.com/aozorabunko/aozorabunko/master/cards/{author_name}/{work_title}.txt"
        ]
        
        for url in possible_urls:
            try:
                response = self.session.head(url, timeout=5)
                if response.status_code == 200:
                    print(f"âœ… ç›´æ¥æ¤œç´¢æˆåŠŸ: {url}")
                    return url
            except:
                continue
        
        print(f"âŒ ç›´æ¥æ¤œç´¢ã§è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {work_title}")
        return None
    
    def download_and_extract_text(self, text_url: str) -> Optional[str]:
        """é’ç©ºæ–‡åº«ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦æ­£è¦åŒ–"""
        if not text_url:
            return None
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        cache_filename = self._get_cache_filename(text_url)
        cache_path = os.path.join(self.cache_dir, cache_filename)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        if os.path.exists(cache_path):
            print(f"ğŸ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿: {cache_filename}")
            with open(cache_path, 'r', encoding='utf-8') as f:
                return f.read()
        
        try:
            print(f"ğŸ“¥ ãƒ†ã‚­ã‚¹ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {text_url}")
            response = self.session.get(text_url, timeout=30)
            response.raise_for_status()
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’ç¢ºèªã—ã¦HTML/ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ¤å®š
            content_type = response.headers.get('content-type', '').lower()
            
            if 'html' in content_type or text_url.endswith('.html'):
                # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
                raw_text = self._extract_text_from_html(response.content)
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
                raw_text = self._decode_content(response.content)
            
            if not raw_text:
                print(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—")
                return None
            
            # é’ç©ºæ–‡åº«è¨˜æ³•ã‚’æ­£è¦åŒ–
            normalized_text = self.normalize_aozora_text(raw_text)
            
            if len(normalized_text) < 100:
                print(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã¾ã™: {len(normalized_text)}æ–‡å­—")
                return None
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            with open(cache_path, 'w', encoding='utf-8') as f:
                f.write(normalized_text)
            
            print(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆå–å¾—å®Œäº†: {len(normalized_text)}æ–‡å­—")
            return normalized_text
            
        except Exception as e:
            print(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _extract_text_from_html(self, content: bytes) -> Optional[str]:
        """HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºãƒ»ãƒ‡ã‚³ãƒ¼ãƒ‰
            html_text = self._decode_content(content)
            if not html_text:
                return None
            
            # BeautifulSoupã§HTMLã‚’ãƒ‘ãƒ¼ã‚¹
            soup = BeautifulSoup(html_text, 'html.parser')
            
            # é’ç©ºæ–‡åº«HTMLã®æœ¬æ–‡éƒ¨åˆ†ã‚’æŠ½å‡º
            # ä¸€èˆ¬çš„ã« div.main_text ã¾ãŸã¯ bodyå†…ã®ãƒ†ã‚­ã‚¹ãƒˆ
            main_text = soup.find('div', class_='main_text')
            
            if not main_text:
                # ä»£æ›¿: bodyã‚¿ã‚°ã‹ã‚‰æŠ½å‡º
                main_text = soup.find('body')
                if main_text:
                    # scriptã‚„styleã‚¿ã‚°ã‚’é™¤å»
                    for tag in main_text(['script', 'style', 'nav', 'header', 'footer']):
                        tag.decompose()
            
            if main_text:
                # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                text = main_text.get_text()
                print(f"âœ… HTMLâ†’ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›å®Œäº†: {len(text)}æ–‡å­—")
                return text
            else:
                print(f"âŒ HTMLæœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
                
        except Exception as e:
            print(f"âŒ HTMLè§£æã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _decode_content(self, content: bytes) -> Optional[str]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡ºã—ã¦ãƒ‡ã‚³ãƒ¼ãƒ‰"""
        # é’ç©ºæ–‡åº«ã¯ä¸»ã«Shift_JIS
        encodings = ['shift_jis', 'utf-8', 'euc-jp']
        
        for encoding in encodings:
            try:
                return content.decode(encoding)
            except UnicodeDecodeError:
                continue
        
        # æœ€å¾Œã®æ‰‹æ®µ: ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–ã—ã¦Shift_JISã§ãƒ‡ã‚³ãƒ¼ãƒ‰
        try:
            return content.decode('shift_jis', errors='ignore')
        except:
            return None
    
    def normalize_aozora_text(self, raw_text: str) -> str:
        """é’ç©ºæ–‡åº«ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–"""
        text = raw_text
        
        # 1. ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»ãƒ•ãƒƒã‚¿ãƒ¼é™¤å»
        text = self._remove_metadata(text)
        
        # 2. ãƒ«ãƒ“è¨˜æ³•å‡¦ç†
        text = self._process_ruby(text)
        
        # 3. æ³¨è¨˜ãƒ»è¨˜æ³•é™¤å»
        text = self._remove_annotations(text)
        
        # 4. æ”¹è¡Œãƒ»ç©ºç™½æ­£è¦åŒ–
        text = self._normalize_whitespace(text)
        
        return text.strip()
    
    def _remove_metadata(self, text: str) -> str:
        """ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ±ã‚’é™¤å»"""
        lines = text.split('\n')
        content_lines = []
        in_content = False
        
        for line in lines:
            line = line.strip()
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼çµ‚äº†ãƒ»æœ¬æ–‡é–‹å§‹ã®æ¤œå‡º
            if not in_content:
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                if (line.startswith('åº•æœ¬ï¼š') or line.startswith('å…¥åŠ›ï¼š') or 
                    line.startswith('æ ¡æ­£ï¼š') or line.startswith('â€»') or
                    '------' in line or line == '' or 
                    'é’ç©ºæ–‡åº«' in line):
                    continue
                else:
                    # æœ¬æ–‡é–‹å§‹
                    in_content = True
            
            # ãƒ•ãƒƒã‚¿ãƒ¼æ¤œå‡ºã§çµ‚äº†
            if in_content and ('åº•æœ¬ï¼š' in line or 'å…¥åŠ›ï¼š' in line or 'æ ¡æ­£ï¼š' in line):
                break
            
            if in_content:
                content_lines.append(line)
        
        return '\n'.join(content_lines)
    
    def _process_ruby(self, text: str) -> str:
        """ãƒ«ãƒ“è¨˜æ³•ã‚’å‡¦ç†"""
        # ï½œæ¼¢å­—ã€Šã‹ã‚“ã˜ã€‹ â†’ æ¼¢å­—
        text = re.sub(r'ï½œ([^ã€Š]+)ã€Š[^ã€‹]+ã€‹', r'\1', text)
        
        # æ¼¢å­—ã€Šã‹ã‚“ã˜ã€‹ â†’ æ¼¢å­—
        text = re.sub(r'([ä¸€-é¾¯]+)ã€Š[^ã€‹]+ã€‹', r'\1', text)
        
        # æ®‹ã£ãŸãƒ«ãƒ“è¨˜å·é™¤å»
        text = re.sub(r'ã€Š[^ã€‹]*ã€‹', '', text)
        text = text.replace('ï½œ', '')
        
        return text
    
    def _remove_annotations(self, text: str) -> str:
        """æ³¨è¨˜ãƒ»è¨˜æ³•ã‚’é™¤å»"""
        # ï¼»ï¼ƒ...ï¼½è¨˜æ³•é™¤å»
        text = re.sub(r'ï¼»ï¼ƒ[^ï¼½]*ï¼½', '', text)
        
        # ã€”...ã€•ç·¨è€…æ³¨é™¤å»
        text = re.sub(r'ã€”[^ã€•]*ã€•', '', text)
        
        # â€»æ³¨é‡ˆè¡Œé™¤å»
        text = re.sub(r'â€»[^\n]*\n', '', text)
        
        # ãã®ä»–ã®è¨˜å·
        text = re.sub(r'ï¼Š', '', text)
        
        return text
    
    def _normalize_whitespace(self, text: str) -> str:
        """ç©ºç™½ãƒ»æ”¹è¡Œã‚’æ­£è¦åŒ–"""
        # Windowsæ”¹è¡Œçµ±ä¸€
        text = re.sub(r'\r\n', '\n', text)
        
        # é€£ç¶šæ”¹è¡Œã‚’2ã¤ã¾ã§ã«åˆ¶é™
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã®é€£ç¶šã‚’1ã¤ã«
        text = re.sub(r'ã€€+', 'ã€€', text)
        
        # è¡Œé ­ãƒ»è¡Œæœ«ã®ã‚¹ãƒšãƒ¼ã‚¹é™¤å»
        lines = text.split('\n')
        lines = [line.strip() for line in lines]
        
        return '\n'.join(lines)
    
    def _get_cache_filename(self, url: str) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ"""
        # URLã‹ã‚‰å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        filename = re.sub(r'[^\w\-_.]', '_', url.split('/')[-1])
        if not filename.endswith('.txt'):
            filename += '.txt'
        return filename
    
    def get_sample_works(self) -> List[Dict]:
        """ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ä½œå“æƒ…å ±"""
        return [
            {
                'author_name': 'å¤ç›®æ¼±çŸ³',
                'title': 'åŠã£ã¡ã‚ƒã‚“',
                'text_url': 'https://www.aozora.gr.jp/cards/000148/files/752_14964.html'
            },
            {
                'author_name': 'èŠ¥å·é¾ä¹‹ä»‹', 
                'title': 'ç¾…ç”Ÿé–€',
                'text_url': 'https://www.aozora.gr.jp/cards/000879/files/127_15260.html'
            },
            {
                'author_name': 'å¤ªå®°æ²»',
                'title': 'èµ°ã‚Œãƒ¡ãƒ­ã‚¹',
                'text_url': 'https://www.aozora.gr.jp/cards/000035/files/1567_14913.html'
            }
        ]
    
    def test_extraction(self, work_info: Dict = None) -> Dict:
        """æŠ½å‡ºæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
        if not work_info:
            work_info = self.get_sample_works()[0]  # åŠã£ã¡ã‚ƒã‚“
        
        author_name = work_info['author_name']
        title = work_info['title']
        
        print(f"\nğŸ§ª é’ç©ºæ–‡åº«æŠ½å‡ºãƒ†ã‚¹ãƒˆ: {author_name} - {title}")
        
        start_time = time.time()
        
        # URLæ¤œç´¢
        text_url = self.search_aozora_work(title, author_name)
        
        if not text_url:
            # ã‚µãƒ³ãƒ—ãƒ«URLã‚’ä½¿ç”¨
            text_url = work_info.get('text_url')
        
        # ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
        if text_url:
            text_content = self.download_and_extract_text(text_url)
        else:
            text_content = None
        
        end_time = time.time()
        
        result = {
            'author_name': author_name,
            'title': title,
            'text_url': text_url,
            'text_length': len(text_content) if text_content else 0,
            'success': text_content is not None,
            'extraction_time': round(end_time - start_time, 2),
            'sample_text': text_content[:200] + '...' if text_content else None
        }
        
        print(f"âœ… ãƒ†ã‚¹ãƒˆçµæœ: {'æˆåŠŸ' if result['success'] else 'å¤±æ•—'}")
        if result['success']:
            print(f"   ãƒ†ã‚­ã‚¹ãƒˆé•·: {result['text_length']}æ–‡å­—")
            print(f"   ã‚µãƒ³ãƒ—ãƒ«: {result['sample_text']}")
        print(f"   å®Ÿè¡Œæ™‚é–“: {result['extraction_time']}ç§’")
        
        return result 
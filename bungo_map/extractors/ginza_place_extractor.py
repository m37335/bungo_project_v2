#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GiNZA NLPåœ°åæŠ½å‡ºå™¨
"""

import spacy
from typing import List, Dict, Tuple
from bungo_map.core.models import Place


class GinzaPlaceExtractor:
    """GiNZAã‚’ä½¿ã£ãŸé«˜åº¦ãªåœ°åæŠ½å‡ºå™¨"""
    
    def __init__(self):
        try:
            self.nlp = spacy.load('ja_ginza')
            print("âœ… GiNZA (ja_ginza) ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
        except OSError:
            print("âš ï¸ ja_ginza ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ja_core_news_smã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            self.nlp = spacy.load('ja_core_news_sm')
            print("âœ… ja_core_news_sm ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
    
    def extract_places_from_text(self, work_id: int, text: str, aozora_url: str = None) -> List[Place]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åœ°åã‚’æŠ½å‡º"""
        places = []
        
        # GiNZAã®åˆ¶é™ï¼ˆç´„49KBï¼‰ã‚’è€ƒæ…®ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
        max_chars = 40000  # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ã‚’è¨­ã‘ã¦40KB
        text_chunks = self._split_text_by_size(text, max_chars)
        
        print(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²: {len(text_chunks)}ãƒãƒ£ãƒ³ã‚¯")
        
        all_sentences = []
        for chunk_idx, chunk in enumerate(text_chunks):
            try:
                doc = self.nlp(chunk)
                chunk_sentences = [sent.text.strip() for sent in doc.sents]
                all_sentences.extend(chunk_sentences)
                print(f"   ãƒãƒ£ãƒ³ã‚¯{chunk_idx + 1}: {len(chunk_sentences)}æ–‡")
            except Exception as e:
                print(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯{chunk_idx + 1}ã®è§£æã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        print(f"ğŸ“„ ç·æ–‡æ•°: {len(all_sentences)}")
        sentences = all_sentences
        
        for i, sentence in enumerate(sentences):
            # å„æ–‡ã‚’è§£æ
            sent_doc = self.nlp(sentence)
            
            # åœ°åå€™è£œã‚’æŠ½å‡ºï¼ˆGiNZAã®ãƒ©ãƒ™ãƒ«ï¼‰
            for ent in sent_doc.ents:
                if ent.label_ in ['Province', 'City', 'County', 'GPE', 'LOC']:  # GiNZAã®åœ°åãƒ©ãƒ™ãƒ«
                    # å‰å¾Œã®æ–‡è„ˆã‚’å–å¾—
                    before_text = sentences[i-1] if i > 0 else ""
                    after_text = sentences[i+1] if i < len(sentences)-1 else ""
                    
                    # ä¿¡é ¼åº¦è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                    confidence = self._calculate_confidence(ent, sentence)
                    
                    place = Place(
                        work_id=work_id,
                        place_name=ent.text,
                        before_text=before_text[:500],  # 500æ–‡å­—ã«åˆ¶é™
                        sentence=sentence,
                        after_text=after_text[:500],   # 500æ–‡å­—ã«åˆ¶é™
                        aozora_url=aozora_url,
                        confidence=confidence,
                        extraction_method="ginza_nlp"
                    )
                    places.append(place)
        
        return self._deduplicate_places(places)
    
    def _calculate_confidence(self, entity, sentence: str) -> float:
        """åœ°åã®ä¿¡é ¼åº¦ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        base_confidence = 0.7
        
        # å®Ÿåœ¨åœ°åã®å¯èƒ½æ€§
        known_places = [
            'æ±äº¬', 'äº¬éƒ½', 'å¤§é˜ª', 'éŒå€‰', 'æ¾å±±', 'æ´¥è»½', 
            'åŒ—æµ·é“', 'ä¹å·', 'å››å›½', 'æœ¬å·',
            'ã‚·ãƒ©ã‚¯ã‚¹', 'ãƒ­ãƒ¼ãƒ', 'ãƒ‘ãƒª', 'ãƒ­ãƒ³ãƒ‰ãƒ³'
        ]
        
        if entity.text in known_places:
            base_confidence += 0.2
        
        # ã€Œå¸‚ã€ã€ŒçœŒã€ã€Œç”ºã€ãªã©ã®æ¥å°¾è¾
        location_suffixes = ['å¸‚', 'çœŒ', 'ç”º', 'æ‘', 'åŒº', 'å³¶', 'å±±', 'å·', 'æµ·', 'æ¹–']
        if any(entity.text.endswith(suffix) for suffix in location_suffixes):
            base_confidence += 0.15
        
        # æ–‡ä¸­ã§ã®ä½ç½®ï¼ˆæ–‡ã®å‰åŠã®æ–¹ãŒåœ°åã®å¯èƒ½æ€§ãŒé«˜ã„ï¼‰
        if sentence.find(entity.text) < len(sentence) * 0.5:
            base_confidence += 0.05
        
        return min(base_confidence, 1.0)
    
    def _deduplicate_places(self, places: List[Place]) -> List[Place]:
        """é‡è¤‡ã™ã‚‹åœ°åã‚’é™¤å»"""
        seen = set()
        unique_places = []
        
        for place in places:
            # åœ°åã¨ä½œå“IDã®çµ„ã¿åˆã‚ã›ã§é‡è¤‡ãƒã‚§ãƒƒã‚¯
            key = (place.work_id, place.place_name)
            if key not in seen:
                seen.add(key)
                unique_places.append(place)
        
        return unique_places
    
    def _split_text_by_size(self, text: str, max_chars: int) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šã‚µã‚¤ã‚ºä»¥ä¸‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        chunks = []
        
        # æ–‡å˜ä½ã§åˆ†å‰²ã‚’è©¦è¡Œï¼ˆã€‚ï¼ï¼Ÿã§åŒºåˆ‡ã‚Šï¼‰
        sentences = text.split('ã€‚')
        
        current_chunk = ""
        for sentence in sentences:
            # æ–‡ã‚’è¿½åŠ ã—ã¦ã‚‚åˆ¶é™ã‚’è¶…ãˆãªã„å ´åˆ
            if len(current_chunk.encode('utf-8')) + len((sentence + 'ã€‚').encode('utf-8')) <= max_chars:
                current_chunk += sentence + 'ã€‚'
            else:
                # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
                if current_chunk:
                    chunks.append(current_chunk)
                
                # æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã‚’é–‹å§‹
                current_chunk = sentence + 'ã€‚'
                
                # å˜ä¸€ã®æ–‡ãŒåˆ¶é™ã‚’è¶…ãˆã‚‹å ´åˆã¯å¼·åˆ¶åˆ†å‰²
                if len(current_chunk.encode('utf-8')) > max_chars:
                    char_chunks = [current_chunk[i:i+max_chars//3] for i in range(0, len(current_chunk), max_chars//3)]
                    chunks.extend(char_chunks[:-1])
                    current_chunk = char_chunks[-1] if char_chunks else ""
        
        # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ 
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def extract_with_context(self, work_id: int, text: str, context_size: int = 50) -> List[Dict]:
        """ã‚ˆã‚Šè©³ç´°ãªæ–‡è„ˆä»˜ãã§åœ°åã‚’æŠ½å‡º"""
        results = []
        doc = self.nlp(text)
        
        for ent in doc.ents:
            if ent.label_ in ['Province', 'City', 'County', 'GPE', 'LOC']:
                # ã‚ˆã‚Šè©³ç´°ãªæ–‡è„ˆæŠ½å‡º
                start_char = max(0, ent.start_char - context_size)
                end_char = min(len(text), ent.end_char + context_size)
                
                context = text[start_char:end_char]
                
                result = {
                    'place_name': ent.text,
                    'start_pos': ent.start_char,
                    'end_pos': ent.end_char,
                    'context': context,
                    'label': ent.label_,
                    'confidence': self._calculate_confidence(ent, context)
                }
                results.append(result)
        
        return results
    
    def test_extraction(self, test_texts: List[str]) -> Dict:
        """æŠ½å‡ºæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
        results = {
            'total_texts': len(test_texts),
            'total_places': 0,
            'extractions': []
        }
        
        for i, text in enumerate(test_texts):
            places = self.extract_places_from_text(work_id=999, text=text)
            
            extraction = {
                'text_id': i + 1,
                'text': text[:100] + "..." if len(text) > 100 else text,
                'places_count': len(places),
                'places': [
                    {
                        'name': place.place_name,
                        'confidence': place.confidence,
                        'method': place.extraction_method
                    }
                    for place in places
                ]
            }
            
            results['extractions'].append(extraction)
            results['total_places'] += len(places)
        
        return results 